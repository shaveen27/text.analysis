---
title: "Short Introduction to text.analysis Package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Short Introduction to text.analysis Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(tibble.print_min = 4L, tibble.print_max = 4L)
library(text.analysis)
set.seed(1234)
```

# Text Analysis Overview

## Introduction

Text analysis techniques such as Natural Language Processing (NLP) can swiftly 
extract relevant information from different corpus datasets such as documents, 
emails, SMS messages, social media and other textual resources. NLP encompasses 
some common techniques for analyzing human language such as tokenization, 
sentiment analysis and text classification.  

Working with text analysis using Natural Language Processing (NLP) involves 
several steps. Here's a general guide:

1. **Problem Definition:** Understand the objective of your text analysis. This 
could be to extract insights, classify text into categories, perform sentiment 
analysis, or something else.

2. **Data Collection:** Gather a corpus of text data relevant to your analysis. 
This could be from various sources such as websites, social media, books, 
articles, or any other text repositories.

3. **Data Preprocessing:** Implement data cleaning, normalization and 
standardization.
   
4. **Exploratory Data Analysis (EDA):** Understand the characteristics of your 
text data through statistical analysis, visualization, and summarization. This 
helps in identifying patterns and gaining insights.

5. **Model Training:** Split your data into training and testing sets. Train 
your chosen model(s) on the training data and tune hyperparameters to optimize 
performance.

6. **Model Selection:** Choose appropriate NLP models based on your problem and 
data characteristics. Common models include: Naive Bayes, Support Vector 
Machines, Random Forest, Logistic Regression.

7. **Model Evaluation:** Evaluate the performance of your trained model(s) using 
appropriate metrics for your task. For example, accuracy, precision, recall, 
F1-score, or Mean Squared Error (MSE) for regression tasks.


This document introduces you to the series of functions of the **text.analysis** 
package that allows a user to analyze a text dataset using some common Natural 
Language Processing techniques, train some models and evaluate their performance
.


## Description of Data

To explore the text analyis process of this package, we will use the dataset 
`SMSSpamCollection`. The SMS Spam Collection is a public set of SMS labeled 
messages that have been collected for mobile phone spam research. 

```{r setup}
library(text.analysis)

#use the dataset from the library
raw_data <- SMSSpamCollection

raw_object <- convert_mail_list(raw_data)

#see description of the dataset
?SMSSpamCollection
```

This raw dataset contains `r nrow(SMSSpamCollection)` observations and 
`r ncol(SMSSpamCollection)` columns: `category` and `message`. The variable 
`category` has two classes of messages: `ham` and `spam`. The variable `message`
shows a ham or spam text based on the `category` variable.

```{r}
#dimension of the dataset
dim(raw_data)

#example of some observation
head(raw_data)

```
For the raw data, there is no missing values and there are 4827 `ham` and 747
`spam` messages. In addition to this, the `min`, `mean` and `max` values for 
the **word count** in the messages for the observations are `1.00`, `15.18` and 
`171.00`, respectively. The `min`, `mean` and `max` values for the 
**message length** for the observations are `2.00`, `77.98` and `910`, 
respectively.

```{r}
#data exploration on the raw data
data_exploration <- explore_data(raw_object)
data_exploration[1:5]

```

## Data Preprocessing

The `message` feature in the raw dataset contains extraneous elements such as 
punctuation marks, whitespace characters, and numeric digits. These superfluous 
components can impede efficient analysis and modeling. Therefore, we want to 
remove these unnecessary elements.

In this phase of the analysis, we perform a comprehensive cleaning of the raw 
dataset by removing numeric values, punctuations, and stop words. We convert the 
text messages to lower case to implement data standardization. Also, we remove 
the white spaces to implement normalization.

```{r}
#Start cleaning
clean_corpus <- lower_case(raw_object)

clean_corpus <- remove_numbers(clean_corpus)

clean_corpus <- remove_punctuations(clean_corpus)

clean_corpus <- remove_whitespaces(clean_corpus)

clean_corpus <- remove_stopwords(clean_corpus)

#head(clean_corpus)

```


Now we explore the clean dataset in a similar way we did for the raw data.

```{r}
#data exploration on the raw data
data_exploration <- explore_data(clean_corpus)
data_exploration[1:5]
```
We see that for the clean data, the `min`, `mean` and `max` values for 
the **word count** in the messages for the observations are `0.00`, `5.79` and 
`51.00`, respectively. The `min`, `mean` and `max` values for the 
**message length** for the observations are `0.00`, `35.05` and `290`, 
respectively. All the values are lower than for the raw data because of the 
cleaning implementation.


## Data Visualization

### Chart visualizatiom

Here are some visualization for the data exploration:

* The pie chart shows the total percentage of both classes: `ham` and `spam`. We 
see that 86.6% are classified as ham messages and 13.4% as spam messages.
* The bar charts show the 10 top more frequent words for the `ham` and `spam` 
messages. 
* For the `ham` messages, we have the following top words in a decreasing order: 
good, day, love, time, ü, lor, today, dont, send, pls. 
* For the `spam` messages, we have the following top words in a decreasing order
free, txt, mobile, text, claim, reply, prize, cash, nokia, send.


``` {r}
#explore clean data
explore_visuals(clean_corpus)
```


### Wordcloud visualization

Word clouds provide a visual representation of the most frequently occurring 
words or terms in a text corpus. This visualization tool will help us quickly 
identify the key words within the text data. It gives us a high-level overview 
of the prominent words. 

We generate the wordcloud visualization for the `ham` and `spam` sets.
  
``` {r}
corpus_data <- split_spamham(clean_corpus)

```

``` {r}
#wordcloud for the ham set
wordcloud_ham(corpus_data$Ham, min_freq = 250)

```

```{r}
#wordcloud for the spam set
wordcloud_spam(corpus_data$Spam, min_freq = 50)

```

By generating wordclouds for different subsets of the text data (ham and spam 
sets), we can compare and contrast the prominent words, potentially revealing 
insights into changing trends or preferences.

As we can see, the 10 top words for both sets appear on these wordcloud 
visualizations as well.


### Splitting Data


```{r}


```

## Methods


```{r}


```

### Naïve Bayes classification  


```{r}


```
 
### Support Vector Machine classification 

```{r}


```

### Random Forest classification model. 


```{r}


```

### Logistic Regression classification model. 

```{r}


```


## Results

```{r}


```

## Comparison of Model


```{r}


```

## Discussion & Conclusions
