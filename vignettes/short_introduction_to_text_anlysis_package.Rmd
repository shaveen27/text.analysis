---
title: "Short Introduction to text.analysis Package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Short Introduction to text.analysis Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(tibble.print_min = 4L, tibble.print_max = 4L)
library(text.analysis)
set.seed(1234)
```

# Text Analysis Overview

## Introduction

Text analysis techniques such as Natural Language Processing (NLP) can swiftly 
extract relevant information from different corpus datasets such as documents, 
emails, SMS messages, social media and other textual resources. NLP encompasses 
some common techniques for analyzing human language such as tokenization, 
sentiment analysis and text classification.  

Working with text analysis using Natural Language Processing (NLP) involves 
several steps. Here's a general guide:

1. **Problem Definition:** Understand the objective of your text analysis. This 
could be to extract insights, classify text into categories, perform sentiment 
analysis, or something else.

2. **Data Collection:** Gather a corpus of text data relevant to your analysis. 
This could be from various sources such as websites, social media, books, 
articles, or any other text repositories.

3. **Data Preprocessing:** Implement data cleaning and standardization.
   
4. **Exploratory Data Analysis (EDA):** Understand the characteristics of your 
text data through statistical analysis, visualization, and summarization. This 
helps in identifying patterns and gaining insights.

5. **Model Training:** Split your data into training and testing sets. Train 
your chosen model(s) on the training data and tune hyperparameters to optimize 
performance.

6. **Model Selection:** Choose appropriate NLP models based on your problem and 
data characteristics. Common models include: Naive Bayes, Support Vector 
Machines, Random Forest, Logistic Regression.

7. **Model Evaluation:** Evaluate the performance of your trained model(s) using 
appropriate metrics for your task. For example, accuracy, precision, recall, 
F1-score, or Mean Squared Error (MSE) for regression tasks.


This document introduces you to the series of functions of the **text.analysis** 
package that allows a user to analyze a text dataset using some common Natural 
Language Processing techniques, train some models and evaluate their performance
.


## Description of Data

To explore the text analyis process of this package, we will use the dataset 
`SMSSpamCollection`. The SMS Spam Collection is a public set of SMS labeled 
messages that have been collected for mobile phone spam research. 

```{r setup}
library(text.analysis)

#use the dataset from the library
raw_data <- SMSSpamCollection

raw_object <- convert_mail_list(data)

#see description of the dataset
?SMSSpamCollection
```

This raw dataset contains `r nrow(SMSSpamCollection)` observations and 
`r ncol(SMSSpamCollection)` columns: `category` and `message`. The variable 
`category` has two classes of messages: `ham` and `spam`. The variable `message`
shows a ham or spam text based on the `category` variable.

```{r}
#dimension of the dataset
dim(raw_data)

#example of some observation
head(raw_data)

```
For the raw data, there is no missing values and there are 4827 `ham` and 747
`spam` messages. In addition to this, the `min`, `mean` and `max` values for 
the **word count** in the messages for the observations are `1.00`, `15.18` and 
`171.00`, respectively. The `min`, `mean` and `max` values for the 
**message length** for the observations are `2.00`, `77.98` and `910`, 
respectively.

```{r}
#data exploration on the raw data
data_exploration <- explore_data(raw_object)
data_exploration[1:5]

```

### Data Preprocessing

As shown, the `message` variable of our raw data contains special characters 
(punctuations, white spaces, numbers, etc.) which need to be removed for making 
our analysis more efficient.

In this part of our analysis, we clean our raw dataset by removing numbers, 
punctuations, white spaces, and stop words. Also, we convert the messages to 
lower case to implement data standardization.

```{r}
#Start cleaning
clean_corpus <- lower_case(raw_object)

clean_corpus <- remove_numbers(clean_corpus)

clean_corpus <- remove_punctuations(clean_corpus)

clean_corpus <- remove_whitespaces(clean_corpus)

clean_corpus <- remove_stopwords(clean_corpus)

```

HERE

### Splitting Data


```{r}


```

## Methods


```{r}


```

### NaÃ¯ve Bayes classification  


```{r}


```
 
### Support Vector Machine classification 

```{r}


```

### Random Forest classification model. 


```{r}


```

### Logistic Regression classification model. 

```{r}


```


## Results

```{r}


```

## Comparison of Model


```{r}


```

## Discussion & Conclusions
